[
  [
    [
      "98414fd1-37b6-4cf7-8110-0308f567ae64",
      {
        "pageContent": "---\ntitle: With JSR\n---\n\nJSR is a new package registry for the JavaScript ecosystem.\n\n**TLDR**\n\nThis is a very simple sample extension https://github.com/kunkunsh/kunkun-ext-ip-info.\nYou can use it as a template.\n\nAfter developing your extension, you can publish it to JSR by following these steps:\n\n## Create a package on `jsr.io/new`\n\n1. Go to https://jsr.io/new, enter a scope (e.g. `kunkun`) and a package name (e.g. `kunkun-ext-ip-info`), and click \"Create Package\".\n2. Link your GitHub repository to the package.\n    ![](../../../../../assets/demo/instructions/jsr-link-github.png)\n\n\n## Add a `jsr.json` file\n\n```json title=\"jsr.json\"\n{\n  \"name\": \"@kunkun/kunkun-ext-ip-info\",\n  \"version\": \"0.0.5\",\n  \"license\": \"MIT\",\n  \"exports\": \"./mod.ts\",\n  \"publish\": {\n    \"include\": [\"dist\", \"README.md\", \"package.json\", \"mod.ts\"]\n  }\n}\n```",
        "metadata": {
          "source": "/Users/hk/Dev/kunkun-docs/src/content/docs/guides/Extensions/Publish/jsr.md",
          "loc": { "lines": { "from": 1, "to": 33 } },
          "sha256": "1a5d9b3ea54168ec8f89e1a93950b1d5c527c831b951070d3ffcce853c6e797f"
        }
      }
    ],
    [
      "70e44c06-e96d-45cc-99e8-808f940f09eb",
      {
        "pageContent": "Although you are not publishing a library, but JSR requires an `exports` field in `jsr.json`, so create an empty `mod.ts` file in the root directory of your project.\n\nInclude the files you want to publish in the `publish.include` field.\n\n`package.json`, `mod.ts` are necessary, and don't forget to include the built artifact of your extension, e.g. `dist` or `build` folder.\n\n## Add a GitHub Action Workflow\n\nIn order to get provenance statements for your package, you must publish it through GitHub Action. \n\n> Of source you can experiment with publishing your package locally with `npx jsr publish` to make sure everything works.\n> After that, bump your package version and publish it through GitHub Action.\n> Any package without provenance statement will be rejected by KK's extension store.",
        "metadata": {
          "source": "/Users/hk/Dev/kunkun-docs/src/content/docs/guides/Extensions/Publish/jsr.md",
          "loc": { "lines": { "from": 1, "to": 13 } },
          "sha256": "0cc8092a90aa3f614a87258d372348706bc2f0d33e9235630849cb0aeec4342e"
        }
      }
    ],
    [
      "009803b4-aaa1-4c00-8420-58e03d6591ec",
      {
        "pageContent": "In the following sample GitHub Action Workflow file, we use `bun` for the entire workflow.\nYou can use whatever you like, `npm`, `node`, `pnpm`, `yarn`. \nYou can even build wasm with rust and publish it to JSR.\n\n`bunx kksh verify --publish` is an optional but recommented step. `kksh` is Kunkun's CLI tool. \nIf the `verify --publish` command fails, the package will not be able to be published to KK's extension store.\n\n:::caution\nWhen publishing a package to JSR, make sure the version in `jsr.json` and `package.json` are the same.\nThis is because Kunkun only uses `package.json` as it's manifest file and use its version field, \nbut the version in `jsr.json` is what is used on JSR, and during the verification process we find your package with package name and version in `jsr.json`.\n\n`bunx kksh verify --publish` will check if the version in `jsr.json` and `package.json` are the same, so it's helpful to include it.\n:::",
        "metadata": {
          "source": "/Users/hk/Dev/kunkun-docs/src/content/docs/guides/Extensions/Publish/jsr.md",
          "loc": { "lines": { "from": 1, "to": 14 } },
          "sha256": "90ef40d14982b96bb2c4c657e17dbf74d8b15f1a36712d2759765c9e5de94995"
        }
      }
    ],
    [
      "ed0d864f-659e-4c39-9a43-d2ed536a9aa3",
      {
        "pageContent": "`bunx kksh verify --publish` will check if the version in `jsr.json` and `package.json` are the same, so it's helpful to include it.\n:::\n\n```yaml title=\".github/workflows/jsr-publish.yml\"\nname: JSR Publish\non:\n  push:\n    branches:\n      - main\n\njobs:\n  publish:\n    runs-on: ubuntu-latest\n\n    permissions:\n      contents: read\n      id-token: write\n\n    steps:\n      - uses: actions/checkout@v4\n      - uses: oven-sh/setup-bun@v2\n      - name: Install dependencies\n        run: bun install\n      - name: Verify Package\n        run: bunx kksh verify --publish\n      - name: Build\n        run: bun run build\n      - name: Publish package\n        run: bunx jsr publish\n```\n\n:::tip\nIf you are using monorepo, your root of extension package may not be the root of your repository.\nThen in GitHub Action you can use `working-directory: ./path-to-extension` under step to tell `jsr` which directory to publish.\n:::\n\n## Register your Extension",
        "metadata": {
          "source": "/Users/hk/Dev/kunkun-docs/src/content/docs/guides/Extensions/Publish/jsr.md",
          "loc": { "lines": { "from": 1, "to": 37 } },
          "sha256": "ec62822e3eeffec14393b16a12d0dcb0920c11ad4fa1a3a6c59252ef39321c57"
        }
      }
    ],
    [
      "d1252769-b655-4dda-92ee-c65b4c81adc6",
      {
        "pageContent": "## Register your Extension\n\nGo to https://kunkun.sh/dashboard/publish-extension/jsr, enter your jsr scope.\nAll your packages will be listed, select the one you want to publish to KK's extension store.\n\nMake sure you login with GitHub, it needs to verify the ownership of the package.\n\n![](../../../../../assets/demo/instructions/jsr-publish.png)",
        "metadata": {
          "source": "/Users/hk/Dev/kunkun-docs/src/content/docs/guides/Extensions/Publish/jsr.md",
          "loc": { "lines": { "from": 1, "to": 8 } },
          "sha256": "53f3b51bad507e0799121d30f1a96cb458d6caacda20ee6bd6447aa7a2041772"
        }
      }
    ],
    [
      "8f5d9f88-4201-4870-867c-8b7cd086ae84",
      {
        "pageContent": "---\ntitle: Extension Publish Workflow\nsidebar:\n  order: 0\n---\n\n## TLDR\n\nTo publish an extension to KK's extension store, follow one of these guides:\n\n1. [Publishing with NPM](./npm)\n2. [Publishing with JSR](./jsr)\n\n## Overview\n\n<div style=\"height: 400px\">\n  <iframe width=\"100%\" style=\"height: 100%;\" src=\"https://www.youtube.com/embed/QPZtUDUGr5s\" title=\"Kunkun: Publish Extension Design\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n</div>\nThis article explains KK's extension publishing system and its core design principles.\n\nThe system is built around two fundamental goals:\n\n1. Open Source Transparency\n2. Security Assurance\n\nAll community extensions must be open source for transparency and security. This requirement means that extensions are hosted on GitHub where they're available for public review and contribution.",
        "metadata": {
          "source": "/Users/hk/Dev/kunkun-docs/src/content/docs/guides/Extensions/Publish/design.md",
          "loc": { "lines": { "from": 1, "to": 26 } },
          "sha256": "bc7551da68e709d023b76979f554252467465066285bb4c7aad07e05be82833c"
        }
      }
    ],
    [
      "e0c5e43f-5d53-4fcd-ae6e-187eaa6067f5",
      {
        "pageContent": "All community extensions must be open source for transparency and security. This requirement means that extensions are hosted on GitHub where they're available for public review and contribution.\n\nWhile KK implements a permission control system to restrict extension capabilities, some extensions may need elevated permissions to function properly. Even with these permission controls in place, we cannot completely guarantee that an extension won't behave maliciously.\n\nThis is one key reason why we require extensions to be open source. However, open source alone doesn't ensure complete transparency and security. For example, while npm packages typically link to their GitHub source code, there's no inherent guarantee that the published package is indeed built from the source code. A malicious actor could modify code locally before publishing to npm.\n\nTo address this security gap, both npm and jsr support provenance statements.\nSee",
        "metadata": {
          "source": "/Users/hk/Dev/kunkun-docs/src/content/docs/guides/Extensions/Publish/design.md",
          "loc": { "lines": { "from": 1, "to": 8 } },
          "sha256": "a2fc83c128ebafcfc444c3740a3afcedc2e92b8c10f51724d41da57b8f670546"
        }
      }
    ],
    [
      "e47c0840-6b15-4c50-bdbc-f07bad9a9777",
      {
        "pageContent": "To address this security gap, both npm and jsr support provenance statements.\nSee\n\n- https://docs.npmjs.com/generating-provenance-statements\n- https://jsr.io/docs/trust\n\n#### NPM Provenance\n\n![](../../../../../assets/demo/instructions/npm-provenance.png)\n\n#### JSR Provenance\n\n![](../../../../../assets/demo/instructions/jsr-provenance.png)\n\nA provenance statement verifies the origin and authenticity of software packages. It creates a verifiable record of where the code came from, how it was built, and who contributed to it. This helps ensure transparency and detect potential security risks or supply chain vulnerabilities.\n\n## Current Implementation\n\nWhile we plan to implement a dedicated provenance system for KK's extension store in the future, we currently leverage the existing provenance systems from npm and jsr.\n\nThe current publishing process works as follows:",
        "metadata": {
          "source": "/Users/hk/Dev/kunkun-docs/src/content/docs/guides/Extensions/Publish/design.md",
          "loc": { "lines": { "from": 1, "to": 21 } },
          "sha256": "04c793bf24afcd608ecbbb3606ba1280602f02c678bb885b2e75f423ce394e22"
        }
      }
    ],
    [
      "94934e87-393a-4f74-91ed-cd7b1490d89f",
      {
        "pageContent": "The current publishing process works as follows:\n\n1. Extensions must be published to either npm or jsr via GitHub Actions\n2. Developers then register their extension through our website\n\nFor detailed publishing instructions, see the following guides:\n\n- [Publishing with NPM](./npm)\n- [Publishing with JSR](./jsr)",
        "metadata": {
          "source": "/Users/hk/Dev/kunkun-docs/src/content/docs/guides/Extensions/Publish/design.md",
          "loc": { "lines": { "from": 1, "to": 9 } },
          "sha256": "3fe4c596a0bedd833fbe214081467647786a12561e3af0e4cd88a43c2787f35f"
        }
      }
    ],
    [
      "4443b394-cd6c-4642-8c18-4e02acc3f29e",
      {
        "pageContent": "---\ntitle: With NPM\n---\n\nNPM is the most popular package manager for JavaScript.\n\nAfter developing your extension, you can publish it to JSR by following these steps:\n\n## Obtain NPM Access Token\n\nRead https://docs.npmjs.com/creating-and-viewing-access-tokens\n\nObtain an NPM access token and store it in your GitHub repository secrets. [Docs](https://docs.github.com/en/actions/security-for-github-actions/security-guides/using-secrets-in-github-actions)\n\n## Create a package on NPM\n\nSuppose your extension will generate a `dist` folder after building.",
        "metadata": {
          "source": "/Users/hk/Dev/kunkun-docs/src/content/docs/guides/Extensions/Publish/npm.md",
          "loc": { "lines": { "from": 1, "to": 17 } },
          "sha256": "95f0df47bfd2b698f43aac7dc22ac13948fe093f838a25ab63c5f6f9b789d942"
        }
      }
    ],
    [
      "71264655-6296-47c6-ba77-0b41039dd320",
      {
        "pageContent": "## Create a package on NPM\n\nSuppose your extension will generate a `dist` folder after building.\n\n1. Make sure your `package.json`'s `files` field includes the `dist` folder as well as everything else you want to publish.\n2. Make sure your `package.json` has a `license` field with a valid license like `MIT`.\n3. Make sure your `package.json` has a `repository` field with a valid GitHub repository URL. (e.g. `\"repository\": \"https://github.com/kunkunsh/kunkun-ext-video-processing\",`)\n   - This repo url is necessary for the provenance statement.\n4. `name` in `package.json` is recommended to start with `kunkun-ext` (no strict requirement).\n5. `npm login` and `npm publish`\n\nThen your package should be available on NPM. `https://www.npmjs.com/package/<package name>`\n\nYou may not be able to find it on NPM by searching for it, but you can access it directly by URL.\n\n## Add a GitHub Action Workflow",
        "metadata": {
          "source": "/Users/hk/Dev/kunkun-docs/src/content/docs/guides/Extensions/Publish/npm.md",
          "loc": { "lines": { "from": 1, "to": 16 } },
          "sha256": "a9b8caefaa7a959928c587f74399a86549b7680894c1c882a631725a5252881e"
        }
      }
    ],
    [
      "8eb2aac7-65de-40bc-a2bf-35971f5a3c15",
      {
        "pageContent": "You may not be able to find it on NPM by searching for it, but you can access it directly by URL.\n\n## Add a GitHub Action Workflow\n\nMake sure `npm publish --provenance` command has the `--provenance` flag. Otherwise provenance statements won't be generated.\n\nMake sure `NPM_TOKEN` is assigned to `NODE_AUTH_TOKEN` environment variable during the publish step.\n\n```yaml title=\".github/workflows/npm-publish.yml\"\nname: NPM Package Publish\n\non:\n  workflow_dispatch:\n\njobs:\n  publish-npm:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      id-token: write\n    steps:\n      - uses: actions/checkout@v4\n      - uses: oven-sh/setup-bun@v2\n      - run: bun install\n      - run: bun run build\n      - name: Publish to NPM\n        run: npm publish --provenance --access public\n        env:\n          NODE_AUTH_TOKEN: ${{secrets.NPM_TOKEN}}\n```\n\n`workflow_dispatch:` means you have to go to GitHub Action tab in your repo and manually trigger the workflow.",
        "metadata": {
          "source": "/Users/hk/Dev/kunkun-docs/src/content/docs/guides/Extensions/Publish/npm.md",
          "loc": { "lines": { "from": 1, "to": 32 } },
          "sha256": "ab68d62cb3631acf57b0704996303f2704730dd27fb30fd867092c8af90f4fb5"
        }
      }
    ],
    [
      "53683cba-b791-4174-8b7a-fb83b3b0e619",
      {
        "pageContent": "`workflow_dispatch:` means you have to go to GitHub Action tab in your repo and manually trigger the workflow.\n\nYou can configure it to run on **release created** event or **push event** to main branch. \n\nIf you let it trigger on push, publishing duplicate version to npm will fail.\n\nTo prevent this, you can check the version of the package in the npm registry before publishing.\nHere is an example of how to do this:\n\n```yaml title=\".github/workflows/npm-publish.yml\"\nname: NPM Package Publish\n\non:\n  push:\n    branches: [main]\n  release:\n    types: [created]\n  workflow_dispatch:\n\njobs:\n  publish-npm:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      id-token: write\n    steps:\n      - uses: actions/checkout@v4\n      - uses: oven-sh/setup-bun@v2\n      - run: bun install\n      - run: bun run build\n      - run: |\n          PACKAGE_NAME=$(jq -r '.name' package.json)\n          PACKAGE_VERSION=$(jq -r '.version' package.json)",
        "metadata": {
          "source": "/Users/hk/Dev/kunkun-docs/src/content/docs/guides/Extensions/Publish/npm.md",
          "loc": { "lines": { "from": 1, "to": 33 } },
          "sha256": "926688c2cc2bf5214f8ad2f82bdd68a630f0226118f50c54f8d618c2417f4c9f"
        }
      }
    ],
    [
      "a57847a1-d9d3-49ca-b4ad-ec699d19733f",
      {
        "pageContent": "# Get the version from npm registry\n          REGISTRY_VERSION=$(npm show \"$PACKAGE_NAME\" version)\n\n          # Compare versions\n          if [ \"$PACKAGE_VERSION\" == \"$REGISTRY_VERSION\" ]; then\n            echo \"Version $PACKAGE_VERSION already exists in the npm registry.\"\n            exit 0\n          else\n            echo \"Version $PACKAGE_VERSION does not exist in the npm registry. Proceeding...\"\n            npm publish --provenance --access public\n          fi\n        env:\n          NODE_AUTH_TOKEN: ${{secrets.NPM_TOKEN}}\n```\n\n## Register your Extension\n\nGo to https://kunkun.sh/dashboard/publish-extension/npm, enter your npm package name.\n\nMake sure you login with GitHub, it needs to verify the ownership of the package.\n\n![](../../../../../assets/demo/instructions/npm-publish.png)",
        "metadata": {
          "source": "/Users/hk/Dev/kunkun-docs/src/content/docs/guides/Extensions/Publish/npm.md",
          "loc": { "lines": { "from": 1, "to": 22 } },
          "sha256": "4e52cee59ad6b3220e31dee7e2c9da499ce2f6c6a6a16305fefc4d05c0d63e08"
        }
      }
    ],
    [
      "bbe73410-cfaa-4098-ae7f-31cc1167980a",
      {
        "pageContent": "Assessing Visually-Continuous Corruption Robustness of Neural Networks\nRelative to Human Performance\nHuakun Shen\nUniversity of Toronto\nhuakunshen@cs.toronto.edu\nBoyue Caroline Hu\nUniversity of Toronto\nboyue@cs.toronto.edu\nKrzysztof Czarnecki\nUniversity of Waterloo\nkczarnec@gsd.uwaterloo.ca\nLina Marsso\nUniversity of Toronto\nlina.marsso@utoronto.ca\nMarsha Chechik\nUniversity of Toronto\nchechik@cs.toronto.edu\nAbstract\nNeural Networks (NNs) have surpassed human accuracy\nin  image  classification  on  ImageNet,  yet  they  often  lack\nrobustness  against  image  corruption,  i.e.,  corruption  ro-\nbustness, with such robustness being seemingly effortless\nfor human perception.  In this paper, we proposevisually-\ncontinuous corruption robustness(VCR) – an extension of\ncorruption robustness to allow assessing it over the wide and\ncontinuous range of changes that correspond to the human\nperceptive quality (i.e., from the original image to the full dis-\ntortion of all perceived visual information), along with two\nnovel human-aware metrics for NN evaluation. To compare\nVCR of NNs with human perception, we conducted exten-\nsive experiments on 14 commonly used image corruptions\nwith 7,718 human participants and state-of-the-art robust\nNN models with different training objectives (e.g., standard,\nadversarial, corruption robustness), different architectures\n(e.g., convolution NNs, vision transformers), and different\namounts of training data augmentation. Our study showed\nthat:  1) assessing robustness against continuous corrup-\ntion can reveal insufficient robustness undetected by existing\nbenchmarks; as a result, 2) the gap between NN and human\nrobustness is larger than previously known; and finally, 3)\nsome image corruptions have a similar impact on human\nperception,  offering opportunities for more cost-effective\nrobustness assessments.\n1. Introduction\nFor Neural Networks (NNs) used in safety-critical do-\nmains, ensuring robustness against potential corruptions is\ncrucial [?]. As NNs in these domains automate tasks usually\nperformed by humans, comparing their robustness to human\nperformance is essential.\nHuman VS NN robustness.Corruption robustness mea-\nsures the average-case performance of an NN or humans\non a set of image corruption functions [?].  Existing stud-\nies, including out-of-distribution anomalies [?], benchmark-\ning [?,?],  and comparison with humans [?,?],  generally\nevaluate robustness against a pre-selected, fixed set of trans-\nformation parameter values that represent varying degrees\nof image corruption.  However, these parameters may not\ncapture how different levels of corruption affect human per-\nception.  For instance, the same parameter can impact vi-\nsual perception differently depending on the image’s bright-\nness [?].  Humans can perceive a continuous spectrum of\nvisual corruptions, from subtle to extreme [?,?], so relying\non fixed parameters may lead to incomplete coverage of the\nfull range of visual corruptions and biased evaluations of NN\nrobustness compared with humans.\nContributions and Outlook.To address these issues, we\nintroducevisually-continuous corruption robustness(VCR),\nfocusing on NN robustness across a continuous range of\nimage corruption levels. We also present two novel human-\naware metrics (HMRI and MRSI) for comparing NN perfor-\nmance with human perception. Our extensive experiments,\ninvolving 7,718 Mechanical Turk participants and 14 com-\nmon image transformations from three sources\n1\n, reveal a\nsignificant robustness gap between NNs and humans. No NN\nfully matches human performance across the entire continu-\nous range of corruption levels in terms of both accuracy and\nprediction consistency, and only a few exceed human perfor-\nmance by a small margin in specific levels of corruption. Our\nexperiments yield insightful findings about the robustness of\nhuman and state-of-the-art (SoTA) NNs concerning accuracy,\ndegrees of visual corruption, and consistency of classifica-\ntion, which can contribute towards the development of NNs\nthat match or surpass human perception.  We also discov-\nered classes of corruption transformations for which humans\nshowed similar robustness (e.g., different types of noise),\n1\nThe number is comparable to 15 corruptions included inIMAGENET-C.",
        "metadata": {
          "source": "/Users/hk/Downloads/WACV_2025_Caroline_Huakun__Copy_.pdf",
          "pdf": {
            "version": "1.10.100",
            "info": {
              "PDFFormatVersion": "1.6",
              "IsAcroFormPresent": false,
              "IsXFAPresent": false,
              "Title": "",
              "Author": "",
              "Subject": "",
              "Keywords": "",
              "Creator": "LaTeX with hyperref",
              "Producer": "pdfTeX-1.40.26",
              "CreationDate": "D:20250127045718Z",
              "ModDate": "D:20250127045718Z",
              "Trapped": { "name": "False" }
            },
            "metadata": null,
            "totalPages": 8
          },
          "loc": { "pageNumber": 1 },
          "sha256": "c28379be7109e76bfccffcf7268b1e767c54e5f279179c773ddb33ba1a83717f"
        }
      }
    ],
    [
      "fb2008e6-b807-4f0c-87af-ee16dd045212",
      {
        "pageContent": "while NNs reacted differently.  Recognizing these classes\ncan contribute to reducing the cost of measuring human ro-\nbustness and elucidating the differences between humans\nand computational models. Our validation set with 14 image\ncorruptions, human robustness data, and the evaluation code\nis provided as a toolbox and a benchmark\n2\n.\n2. Related Work\nWe briefly review related work on the comparison of hu-\nman and NN robustness, adversarial robustness, robustness\nbenchmarks and improving robustness.\nHuman VS NN Robustness.Prior studies have used hu-\nman performance to study the existing differences between\nhumans and neural networks [?,?], to study invariant trans-\nformations [?], to compare recognition accuracy [?,?], to\ncompare robustness against image transformations [?,?], or\nto specify expected model behaviour [?]. The main differ-\nence between our study and existing work, specifically, the\nmost recent study by [?], is three-fold:  1) we are the first\nto quantify robustness across the full continuous visual cor-\nruption range, thus revealing previous undetected robustness\ngap; 2) our experiments for obtaining human performance\nare designed to include more participants for measuring the\naveragehuman robustness, resulting in more generalizable\nresults and reduced influence of outliers; 3) we identified\nvisually similar transformations for humans but not NNs,\npotentially reducing experiment costs.\nRobustness  Benchmarks.Hendrycks  et  al.    built  the\nIMAGENET-Cand-Pbenchmarks for checking NN model\nclassification robustness against common corruptions and\nperturbations onIMAGENETimages [?]. They have inspired\nother benchmarks for different corruption functions, datasets,\nand tasks [?,?,?,?,?,?,?]. However, these benchmarks gen-\nerate images by applying corruption functions with only five\npre-selected values per parameter.IMAGENET-CCC[?] is\nthe only prior work targeting a more continuous range of\ncorruptions, by using 20 pre-selected values per parameter.\nIt does not check the coverage in terms of the visual effects\non the images, which we do with an Image Quality Assess-\nment (IQA) metric Visual Information Fedility (VIF) [?].\nFurther, their work focuses on continuous changes over time\nfor benchmarking test-time adaptation, which is different\nfrom a general robustness benchmark, and the dataset has\nnot been released as the time of writing.  In contrast to all\nthese previous works, our method randomly and uniformly\nsamples parameter values to cover the full range of visual\nchange that a corruption function can achieve, which is mod-\neled and assessed for coverage using an IQA metric.  Our\nwork also compares robustness of NNs with humans.\nAdversarial Robustness.Adversarial robustness measures\nthe worst-case performance on images with added ‘small’\ndistortions or perturbations tailored to confuse a classifier [?].\n2\nhttps://github.com/HuakunShen/VCR\nHowever, changes that can be encountered in the real-world\nsituations are often of a much bigger range [?].  Thus, in\nthis paper, we focus onaverage-case performanceover a\nrealisticrange of changes.\nImproving Robustness.Numerous methods for improving\nmodel robustness have been proposed, e.g., data augmenta-\ntion with corrupted data [?,?,?,?], texture changes [?,?], im-\nage compositions [?,?] and corruption functions [?,?]. All of\nthese have different abilities to generalize to unseen data [?].\nWhile not our primary focus, we demonstrate that NN ro-\nbustness compared to humans can be improved through data\naugmentation and fine-tuning with our generated images for\nVCR.\n3. Visually-Continuous Corruption Robustness\nTo study robustness against a wide and continuous spec-\ntrum of visual changes, we definevisually-continuous cor-\nruption robustness(VCR) and describe our method for gen-\nerating test sets. To study VCR of NNs in relation to humans,\nwe also present the human-aware metrics.\n3.1. VCR Definition\nA key difference between corruption robustness and VCR\nis  that  the  latter  is  defined  relative  to  thevisual  impact\nof image corruption on human perception, rather than the\ntransformation parameter domain.  To quantify visual cor-\nruption,  VCR uses the Image Quality Assessment (IQA)\nmetric Visual Information Fidelity (VIF) [?,?].  VIF mea-\nsures the perceived quality of a corrupted imagex\n′\ncom-\npared to its original formxby measuring the visual infor-\nmation unaffected by the corruption.  Thus, we define the\nchangein the perceived quality caused by the corruption as\n∆\nv\n(x,x\n′\n) =max(0,1−VIF(x,x\n′\n)). See App.??for more\ndetail on∆\nv\n. With∆\nv\n, whose value ranges from 0 and 1, we\ncan consider VCR against the wide, finite, and continuous\nspectrum of visual corruptions ranging from no degradation\nto visual quality (i.e., the original image) (∆\nv\n= 0) to the\nfull distortion of all visual information (∆\nv\n= 1).\nLimitation:VCR is limited to image corruption applicable to\nthe chosen IQA metric, thus by using VIF, VCR is limited to\nonly pixel-level corruption. Metrics suitable for other types\nof corruption (e.g., geometric) need further research.\nFor  VCR,  we  consider  a  classifier  NNf:X→Y\ntrained on samples of a distribution of input imagesP\nX\n,\na ground-truth labeling functionf\n∗\n, and a parameterized\nimage corruption functionT\nX\nwith a parameter domainC.\nWe are interested in robustness offagainst images with all\ndegrees of visual corruptionuniformlyranging from∆\nv\n= 0\nto∆\nv\n= 1.\n3\nGiven a valuev∈[0,1], we defineP(x,x\n′\n|v)\nas thejoint distributionof original images (x) and corre-\nsponding corrupted images (x\n′\n=T\nX\n(x,c),c∈C) with\n3\nNote that distributions other than uniform can be used based on the\napplication. For example, one may wish to favour robustness against heavy\nsnow conditions for NNs deployed in arctic areas.",
        "metadata": {
          "source": "/Users/hk/Downloads/WACV_2025_Caroline_Huakun__Copy_.pdf",
          "pdf": {
            "version": "1.10.100",
            "info": {
              "PDFFormatVersion": "1.6",
              "IsAcroFormPresent": false,
              "IsXFAPresent": false,
              "Title": "",
              "Author": "",
              "Subject": "",
              "Keywords": "",
              "Creator": "LaTeX with hyperref",
              "Producer": "pdfTeX-1.40.26",
              "CreationDate": "D:20250127045718Z",
              "ModDate": "D:20250127045718Z",
              "Trapped": { "name": "False" }
            },
            "metadata": null,
            "totalPages": 8
          },
          "loc": { "pageNumber": 2 },
          "sha256": "451ecba7690493bf70d26bf127bff2de09d7d5f0b51c16c363f3f372184a2988"
        }
      }
    ],
    [
      "c01af585-d8b9-4819-9a7a-ed21c9890e0a",
      {
        "pageContent": "∆\nv\n(x,x\n′\n) =v. We define VCR in the presence of a robust-\nness propertyγthatfshould satisfy givenT\nX\n:\nR\nγ\n=E\nv∼Uniform(0,1)\n(P\nx,x\n′\n∼P(x,x\n′\n|v)\n(γ))(1)\nIn this paper, we instantiate VCR with two existing robust-\nness properties (see Fig. 1). The first one isaccuracy (a\nv\n),\nrequiring that the prediction on corrupted images should be\ncorrect, i.e.,f(x\n′\n) =f\n∗\n(x).  It is also used in the existing\ndefinition of corruption robustness [?]. Thus,\nR\na\n=E\nv∼Uniform(0,1)\n(P\nx,x\n′\n∼P(x,x\n′\n|v)\n(f(x\n′\n) =f\n∗\n(x)))(2)\nThe second property isprediction consistency (p\nv\n), requir-\ning consistent predictions before and after corruptions, i.e.,\nf(x\n′\n) =f(x)[?]. It is applicable when ground truth is not\navailable, which is common during deployment. Thus,\nR\np\n=E\nv∼Uniform(0,1)\n(P\nx,x\n′\n∼P(x,x\n′\n|v)\n(f(x\n′\n) =f(x)))(3)\nSummary of VCR Definitions. Fig. 1 gives a visual sum-\nmary of the VCR metrics, starting with the general definition\nR\nγ\nat the top, and instantiating it for accuracy asR\na\nand\nconsistency asR\np\n. Each of them is the average accuracy or\nprediction consistency, respectively, over the full and contin-\nuous range of visual change∆\nv\n∈[0,1].\n3.2. Testing VCR\nVCR of a subject (a human or an NN) is measured by\nfirst generating a test set through sampling and then esti-\nmating  it  using  the  sampled  data.   The  test  set  is  gener-\nated by sampling images and applying corruption to obtain\nP(x,x\n′\n|v)for different∆\nv\nvaluesv.  We samplex∼P\nX\nandc∼Uniform(C),  and  obtainx\n′\n=T\nX\n(x,c)and\nv= ∆\nv\n(x,x\n′\n), resulting in samples(x,x\n′\n,c,v). Then, we\ndivide them into groups of(x,x\n′\n,c), each with the same\nvvalue.  Next, by droppingc, we obtain groups of(x,x\n′\n)\nwith the samev, which are samples fromP(x,x\n′\n|v). Note\nthat we consider each group separately, thus this procedure\nrequires only sufficient data in each group but not unifor-\nmity, i.e.,v∼Uniform(0,1)is not required.  The varying\nsize of each group, i.e., the non-uniformity ofvdistribution,\nwill not distort VCR estimates, but only impact the estimate\nuncertainty at a givenv.  Further, interpolation in the next\nstep helps address any missing points.\nWith the test set, we estimate the performance w.r.t. the\npropertyγfor eachv. For eachvin the test data, we compute\ntherateof accurate predictionsf(x\n′\n) =f\n∗\n(x)to estimate\naccuracy, i.e.,a\nv\n=P\nx,x\n′\n∼P(x,x\n′\n|v)\n(f(x\n′\n) =f\n∗\n(x))[resp.\nconsistent predictionsf(x\n′\n) =f(x)to estimate consistency,\ni.e.,p\nv\n=P\nx,x\n′\n∼P(x,x\n′\n|v)\n(f(x\n′\n)  =f(x))].  Then by plot-\nting(v,a\nv\n)and(v,p\nv\n)and applying monotonic smooth-\ning splines [?] to reduce randomness and outliers, we ob-\ntain smoothed spline curvess\na\nands\np\n, respectively.  The\ncurvess\nγ\n(namely,s\na\nands\np\n)  describe  how  the  perfor-\nmance w.r.t.  the robustness propertyγ(namely,aandp)\ndecreases as the visual corruption in images increases.  Fi-\nnally, we estimateR\na\n=E\nv∼Uniform(0,1)\n(a\nv\n)[resp.R\np\n=\nE\nv∼Uniform(0,1)\n(p\nv\n)] as the area under the spline curve, i.e.,\nˆ\nR\na\n=A\na\n=\nR\n1\n0\ns\na\n(v)dv[resp.\nˆ\nR\np\n=A\np\n=\nR\n1\n0\ns\np\n(v)dv].\nSee Alg.??for the pseudo-code of VCR estimation.\n3.3. Human-Aware Metrics for VCR\nA commonly used metric for measuring corruption ro-\nbustness is theCorruption Error (CE)[?]—the top-1 classifi-\ncation error rate on the corrupted images, normalized by the\nerror rate of a baseline model. CE can be used to compare\nan NN with humans if the baseline model is set to be hu-\nmans. However, CE is not able to determine whether an NN\ncan exceed humans, and NN models could potentially have\nsuper-human accuracy for particular types of perturbations\nor in some∆\nv\nranges. Inspired by CE, we propose two new\nhuman-aware metrics,Human-Relative Model Robustness\nIndex(HMRI) that measures NN VCR relative to human\nVCR; andModel Robustness Superiority Index(MRSI) that\nmeasures how much an NN exceeds human VCR.\nAuxiliary VCR metrics to computeHMRIandMRSI.\nHMRIandMRSItake as inputs the estimated spline curves\nfor humans (s\nh\nγ\n) and for NN (s\nm\nγ\n).  We denote areas under\nthese curves asA\nh\nγ\nandA\nm\nγ\n, respectively (see Fig. 2a).  To\ncompare NN model and human performance, VCR w.r.t. pre-\ndiction consistency or accuracy is estimated using Alg.??\nusing both model and human performance data, as illustrated\nby the yellow (A\nh\nγ\n) and blue (A\nm\nγ\n) areas in Fig. 2a, respec-\ntively. Both the blue and yellow areas also include the green\narea representing their overlap. Additionally, the VCR lead\nof humans over a modelA\nh>m\nγ\n, the girded area in Fig. 2a,\nand the VCR lead of a model over humansA\nm>h\nγ\n, the striped\narea in Fig. 2a, are estimated. The definitions of these four\nauxiliary metrics are summarized in Tab. 2b, and they are\nused to defineHMRIandMRSI.\nDefinition 1(HMRI).asdaisaoshdoaishd asdasdqw\nGivens\nh\nγ\nands\nm\nγ\n, letA\nh>m\nγ\n=\nR\n1\n0\n(s\nh\nγ\n(v)−s\nm\nγ\n(v))\n+\ndv\nde-\nnote the average (accuracy or preservation) performance\nlead  of  humans  over  a  model  across  the  visual  change\nrange, where the performance lead is defined as the positive\npart of performance difference, i.e.,(s\nh\nγ\n(v)−s\nm\nγ\n(v))\n+\n=\nmax(0,s\nh\nγ\n(v)−s\nm\nγ\n(v)).  Human-Relative Model Robust-\nness Index (HMRI), which quantifies the extent to which\na  DNN  can  replicate  human  performance,  is  defined  as\nA\nh\nγ\n−A\nh>m\nγ\nA\nh\nγ\n= 1−\nA\nh>m\nγ\nA\nh\nγ\n.\nTheHMRIvalue ranges from[0,1]; a higherHMRIindicates\na NN model closer to human VCR, andHMRI= 1signifies\nthats\nm\nγ\nis the same as or completely aboves\nh\nγ\nin the entire\n∆\nv\ndomain, i.e., the NN is at least as robust as an average\nhuman (see Fig. 2a).\nDefinition  2(MRSI).Givens\nh\nγ\nands\nm\nγ\n,  letA\nm>h\nγ\n=\nR\n1\n0\n(s\nm\nγ\n(v)−s\nh\nγ\n(v))\n+\ndvdenote the average performance\nlead  of  a  model  over  a  human  across  the  visual  change\nrange. Model Robustness Superiority Index (MRSI), which",
        "metadata": {
          "source": "/Users/hk/Downloads/WACV_2025_Caroline_Huakun__Copy_.pdf",
          "pdf": {
            "version": "1.10.100",
            "info": {
              "PDFFormatVersion": "1.6",
              "IsAcroFormPresent": false,
              "IsXFAPresent": false,
              "Title": "",
              "Author": "",
              "Subject": "",
              "Keywords": "",
              "Creator": "LaTeX with hyperref",
              "Producer": "pdfTeX-1.40.26",
              "CreationDate": "D:20250127045718Z",
              "ModDate": "D:20250127045718Z",
              "Trapped": { "name": "False" }
            },
            "metadata": null,
            "totalPages": 8
          },
          "loc": { "pageNumber": 3 },
          "sha256": "f0b6f639135d28f12914f4fd9d8e7f11aa3c67e736cbbfbda36ec97f382b4577"
        }
      }
    ],
    [
      "8a4bdf4b-8688-4186-9675-5ded43ae1947",
      {
        "pageContent": "Figure 1. Summary of VCR definitions with respect to accuracy and consistency.\nquantifies the extent to which a DNN model can surpass\nhuman performance, is defined as\nA\nm>h\nγ\nA\nm\nγ\n.\nTheMSRIvalue ranges from[0,1), with the higher value\nindicating  better  performance  than  humans.MSRI=  0\nmeans that the given NN model performs worse than or\nequal to humans in the entire∆\nv\ndomain. A positiveMSRI\nvalue indicates that the given NN model performs better\nthan humans at least in some ranges of∆\nv\n(see Fig. 2a).\nComparing humans and NNs withHMRIandMRSIyields\nthree  possible  scenarios:  (1)  humans’  performance  fully\nexceeds NN’s,  i.e.,0<HMRI<1andMRSI=  0;  (2)\nNN’s performance fully exceeds humans’, i.e.,HMRI= 1\nandMRSI>0;  and (3) humans’ performance is better\nthan NN’s in some∆\nv\nintervals and worse in others, i.e.,\nHMRI<1andMRSI>0.\n4. Experiments\nIn this section, we describe experiments that check the\nVCR of NN models against human performance.\nNN models.Tab. 1 summarizes models included in our study.\nWe have selected a wide range of architectures (CNN and\ntransformer architectures) and training methods (supervised,\nadversarial,  semi-weakly, and self-supervised),  including\ndinov2giant [?], which is on the top of theIMAGENET-C\nleaderboard as of time of writing.  In total, we studied 11\nstandard supervised models, 4 adversarial learning models,\n2 SWSL models, 1 CLIP (clip-vit-base-patch32) model and\n3 ViT models. For CLIP, we used the prompt “a picture of\n(ImageNet class)” while tokenizing the labels.\nImage Corruptions.As shown in Fig. 3, we focus on study-\ning VCR of NNs in relation to humans regarding 14 com-\nmonly used image corruptions from three different sources:\nShot Noise,  Impulse Noise,  Gaussian Noise,  Glass Blur,\nGaussian Blur, Defocus Blur, Motion Blur, Brightness and\nFrost fromIMAGENET-C[?]; Blur, Median Blur, Hue Satu-\nration Value and Color Jitter from Albumentations [?]; and\nUniform Noise from [?].\nCrowdsourcing.Since VCR focuses on the average-case\nperformance, we used crowdsourcing to measure human per-\nformance, as it allows for a larger participant pool and more\naccurate estimation.   The experiment is designed follow-\ning [?] and [?]. The experiment procedure is aforced-choice\nimage categorization task: humans were shown one image\nat a time for 200 ms to limit recurrent processing and asked\nto choose the correct category from 16 entry-level labels [?].\nFor NN models, the 1,000-class decision vector was mapped\nto the same 16 classes using the WordNet hierarchy [?]. The\ntime to classify each image was set to ensure fairness in the\ncomparison between humans and machines [?].  Between\nimages, we showed a noise mask to minimize feedback in-\nfluence in the brain [?]. Qualification tests and sanity checks\nwere used to filter out misunderstandings and instances of\nspam [?], resulting in7,718participants with70,000predic-\ntions on corrupted images and50,000on original images.\nThe same image was never shown to a participant more than\nonce.\n4.1. Testing Robustness against Visual Corruption\nIMAGENET-Cis a SoTA benchmark for corruption ro-\nbustness,  using 5 pre-selected parameter values for each\ncorruption type onIMAGENETvalidation images [?]. This\nsection compares robustness results fromIMAGENET-Cwith\nthose from VCR across all 9IMAGENET-Ccorruption func-\ntions in our study. Full results are available in the Appendix\ndue to page limitations.\nVisual Corruption in Test Sets.For each corruption, our\ngenerated tests contain50,000images, mirroring the size\nof theIMAGENET[?] validation set, whileIMAGENET-C\nincludes5×50,000images. Due to differences in test set\ngeneration, the corruption distributions differ in coverage\nand peak at different values (e.g., Fig. 4).\nTo assess the coverage of∆\nv\nin the test sets,  Tab. 2 shows\nthe percentage of the full∆\nv\ncovered.  The distribution is\ndivided into 40 equal-width bins, with coverage defined as\nhaving 20 or more images per bin.IMAGENET-Cexhibits\nlow∆\nv\ncoverage, particularly for Gaussian blur at 56.4%,\nfocusing mainly on the center and missing the low and high\n∆\nv\nvalues, leading to biased evaluation ( Fig. 4 and  Tab. 2.\nIn  contrast,  our  test  sets  cover  nearly  the  entire  domain,\nwith 97.4% coverage. Our test sets have consistently higher\ncoverage thanIMAGENET-Cfor other corruptions as well.\nFull details are in the Appendix.\nAmong all corruptions studied, Shot Noise and Impulse\nNoise have relatively low coverage,  because the level of\nnoise these functions add is exponential to their parameters.\nAs a result, uniform sampling of the parameter rangeCfails\nto cover small∆\nv\nvalues.  When using uniform sampling\noverC, reaching the full coverage of∆\nv\nwould require a",
        "metadata": {
          "source": "/Users/hk/Downloads/WACV_2025_Caroline_Huakun__Copy_.pdf",
          "pdf": {
            "version": "1.10.100",
            "info": {
              "PDFFormatVersion": "1.6",
              "IsAcroFormPresent": false,
              "IsXFAPresent": false,
              "Title": "",
              "Author": "",
              "Subject": "",
              "Keywords": "",
              "Creator": "LaTeX with hyperref",
              "Producer": "pdfTeX-1.40.26",
              "CreationDate": "D:20250127045718Z",
              "ModDate": "D:20250127045718Z",
              "Trapped": { "name": "False" }
            },
            "metadata": null,
            "totalPages": 8
          },
          "loc": { "pageNumber": 4 },
          "sha256": "a7516e047d1287385581727d5b019512288207bd2172bd241a0c6255d2b570c0"
        }
      }
    ],
    [
      "d2c78f03-36a2-4906-8fa6-3a2b4544b611",
      {
        "pageContent": "(a) Visualization of auxiliary metrics for model\nvs. human performance.\nAuxiliary metric(cf. Fig. 2a)\nVCR of humansw.r.t. a propertyγ,\nestimated as an area under performance curveA\nh\nγ\n:\nˆ\nR\nh\nγ\n=A\nh\nγ\n=\nR\n1\n0\ns\nh\nγ\n(v)dv\nVCR of a modelf(x)w.r.t. a propertyγ,\nestimated as an area under performance curveA\nm\nγ\n:\nˆ\nR\nm\nγ\n=A\nm\nγ\n=\nR\n1\n0\ns\nm\nγ\n(v)dv\nVCR lead of humans over a modelf(x)w.r.t. a propertyγ,\nestimated as a difference areaA\nh>m\nγ\n:\nˆ\nR\nh>m\nγ\n=A\nh>m\nγ\n=\nR\n1\n0\nmax(0,s\nh\nγ\n(v)−s\nm\nγ\n(v))dv\nVCR lead of a modelf(x)over humansw.r.t. a propertyγ,\nestimated as a difference areaA\nm>h\nγ\n:\nˆ\nR\nm>h\nγ\n=A\nm>h\nγ\n=\nR\n1\n0\nmax(0,s\nm\nγ\n(v)−s\nh\nγ\n(v))dv\n(b) Summary of auxiliary metrics for definingHMRIandMRSI.\nFigure 2. Auxiliary VCR metrics to computeHMRIandMSRI.\nModelArchitectureTraining MethodModelArchitectureTraining Method\nNOISYMIX[?]ResNet-50SupervisedNOISYMIXNEW[?]ResNet-50Supervised\nSIN [?]ResNet-50SupervisedSININ [?]ResNet-50Supervised\nSINININ [?]ResNet-50SupervisedHMANY[?]ResNet-50Supervised\nHAUGMIX[?]ResNet-50SupervisedSTANDARDR50 [?]ResNet-50Supervised\nALEXNET[?]AlexNetSupervisedTIANDEIT-S [?]DeiT SmallSupervised ViT\nTIANDEIT-B [?]DeiT BaseSupervised ViTDO502LINF[?]WideResNet-50-2Adversarial\nLIUSWIN-L [?]Swin-LAdversarialLIUCONVNEXT-L [?]ConvNeXt-LAdversarial\nSINGHCONVNEXT-L-CONVSTEM[?]ConvNeXt-L + ConvStemAdversarialSWSLRESNET18 [?]ResNet-18Semi-weakly sup.\nSWSLRESNEXT10132X16D[?]ResNext-101Semi-weakly sup.CLIP [?]ClipSupervised CLIP\nDINOV2GIANT[?]ViTSelf-supervised ViT\nTable 1. Summary of the models included in our study.\nnoise\n(a)Impulse Noise\n(IMAGENET-C)\n(b)Shot Noise\n(IMAGENET-C)\n(c)Gaussian Noise\n(IMAGENET-C)\n(d)Uniform Noise\n( [?])\nblur\n(e)Blur\n(Albumentation)\n(f)Median Blur\n(Albumentation)\n(g)Glass Blur\n(IMAGENET-C)\n(h)Gaussian Blur\n(IMAGENET-C)\n(i)Defocus Blur\n(IMAGENET-C)\nothers\n(j)Motion Blur\n(IMAGENET-C)\n(k)Hue Saturation Value\n(Albumentation)\n(l)Color Jitter\n(Albumentation)\n(m)Brightness\n(IMAGENET-C)\n(n)Frost\n(IMAGENET-C)\nFigure 3. Image corruption functions.\nlarge amount of data. Note, however, Alg.??still computes\nVCR over the full∆\nv\nrange of[0..1], and the lack of sam-\nples for low values of∆\nv\nhas a limited impact on the VCR\nestimate. This is because we fit a monotonic spline that is\nanchored with a known initial performance for∆\nv\n= 0, as\ndiscussed in App.??.\nRemark:\nThe reported accuracy ofIMAGENET-Ccan be\ndirectly impacted both by a lack of coverage and by non-\nuniformity, as it is computed as the average accuracy of all\ncorrupted images. In contrast, the shape of the∆\nv\ndistribu-\ntion in the test images does not impact VCR once sufficient\ncoverage is achieved to estimate the spline curvess\nγ\n.\nRobustness  Evaluation  Results.Next,  we  compare  ro-\nbustness evaluation results obtained withIMAGENET-Cand\nVCR test sets. Consider results for Gaussian Noise in Fig. 5.\nNOISYMIXandNOISYMIXNEWhave almost the same ro-\nbust accuracy onIMAGENET-C, butNOISYMIXNEWhas\nhigher\nˆ\nR\na\n; similarly, SIN has higherIMAGENET-Crobust\naccuracy but lower\nˆ\nR\na\nthan SINININ due to the almost\n(a)IMAGENET-C.(b)VCR Test Set.\nFigure  4.Histograms  showing∆\nv\ndistribution  between\nIMAGENET-C and our VCR test sets for Gaussian Blur.\nCorruption\nCoverage\nIMAGENET-CVCR Test Set\nBrightness0.5901.000\nGaussian Blur0.5640.974\nDefocus Blur0.5380.923\nShot Noise0.4620.590\nFrost0.4361.000\nGaussian Noise0.4360.872\nImpulse Noise0.3850.641\nMotion Blur0.3330.974\nGlass Blur0.3330.949\nTable 2.∆\nv\nCoverage Comparison with IMAGENET-C.",
        "metadata": {
          "source": "/Users/hk/Downloads/WACV_2025_Caroline_Huakun__Copy_.pdf",
          "pdf": {
            "version": "1.10.100",
            "info": {
              "PDFFormatVersion": "1.6",
              "IsAcroFormPresent": false,
              "IsXFAPresent": false,
              "Title": "",
              "Author": "",
              "Subject": "",
              "Keywords": "",
              "Creator": "LaTeX with hyperref",
              "Producer": "pdfTeX-1.40.26",
              "CreationDate": "D:20250127045718Z",
              "ModDate": "D:20250127045718Z",
              "Trapped": { "name": "False" }
            },
            "metadata": null,
            "totalPages": 8
          },
          "loc": { "pageNumber": 5 },
          "sha256": "d67a47978e51dec70271dde449d9d38695a73c9787432379ab13a3b4ef78d7eb"
        }
      }
    ],
    [
      "bcb1ce57-254c-4e78-a735-be0cc4fd988e",
      {
        "pageContent": "complete lack of coverage for∆\nv\n<0.5for Gaussian Noise\ninIMAGENET-C(see Tab. 2), which can lead to biased eval-\nuation results (i.e., biased towards∆\nv\n≥0.5).  Checking\nVCR allows us to detect such biases.\nIn addition to accuracy,  VCR can also check whether\nthe NN can preserve its predictions after corruption,  i.e.,\nthe prediction consistency propertyp\nv\n, giving additional in-\nformation about NN robustness.  Fig. 5b and Fig. 5c show\nthat the modelTIANDEITB has a higher\nˆ\nR\na\nthanSINGH-\nCONVNEXT-L-CONVSTEMbut a lower\nˆ\nR\np\n. This suggests\nthat even thoughTIANDEITB has better accuracy for cor-\nrupted images, it labels the same image with different labels\nbefore and after the corruption. Since ground truth can be\nhard to obtain during deployment, having low prediction con-\nsistency indicates issues with model stability and could raise\nconcerns about when to trust the model prediction. Results\nfor the remaining corruptions are in App.??.\nSummary:\nRobustness must be tested before deploying NNs\ninto an environment with a wide and continuous range of\nvisual corruptions. Our results confirmed thattesting robust-\nness in this range using a fixed and pre-selected number\nof parameter values can lead to undetected robustness\nissues, which can be avoided by checking VCR. Also,accu-\nracy cannot accurately represent model stability when\nfacing corruptions, which can be addressed by testingR\np\n.\n4.2. VCR of DNNs Compared with Humans\nIn this experiment, we useHMRIandMRSImetrics and\nthe data from the human experiment data to compare VCR\nof the studied models against human performance.\nFor Gaussian Noise, Fig. 6 shows our measuredHMRI\nandMRSIvalues forR\na\nandR\np\n, where higher values indi-\ncate better robustness. Fig. 6a reveals that no NN achieves\n1.0forHMRI\na\n, and in Fig. 6d, only 3 out of 21 NNsDI-\nNOV2GIANT, TIANDEIT-BandSINGH-CONVNEXT-L-\nCONVSTEMreached1.0forHMRI\np\n, indicating that there\nare still unclosed gaps between human and NN robustness.\nThese three top-performing models have also the highest\nHMRIvalues for bothR\na\nandR\np\n,  making them closest\nto human robustness.  In  Fig. 6b, these three models have\nMRSI\na\nvalues above0.0, indicating that they surpass human\naccuracy in certain ranges of visual corruption.  This can\nbe visualized by checking the estimated curvess\na\nas shown\nin  Fig. 6c.  The top-three models exceed human accuracy\n(the red curve) when∆\nv\n>0.85. For prediction consistency,\nFig. 6e shows that all NNs have theMRSI\np\nvalue above0.0\nand this is because, as shown in Fig. 6f, all NN curves are\nabove the human curve when the∆\nv\nvalue is small.\nSimilarly, for Uniform Noise, as shown in Fig. 7a and\nFig. 7d, no models reached1.0forHMRI\na\nand the top-three\nmodels, reached1.0forHMRI\np\n. Together with Fig. 7b and\nFig. 7e, we can see that for bothR\na\nandR\np\n, TIANDEIT-B\nhas higherHMRIvalues butTIANDEIT-Shas higherMRSI\nvalues. This suggests that whileTIANDEIT-Bis closer to\nhuman performance,TIANDEIT-Sexceeds human perfor-\nmance more. This counter-intuitive result can be explained\nwith the curvess\na\nands\np\nrepresenting how the performance\nw.r.t. the robustness propertiesaandpdecreases as∆\nv\nin-\ncreases, as shown in Fig. 7c and Fig. 7f.  Based ons\na\nand\ns\np\n, TIANDEIT-Bperforms better thanTIANDEIT-Swhen\n∆\nv\n<0.8, resulting in a higherHMRI. However it performs\nworse and drops more rapidly when∆\nv\n>0.8, leading to a\nlowerMRSI. This suggests that bothHMRIandMRSIare\nuseful for comparing NN robustness, and our curvess\na\nand\ns\np\ncan provide further information on NN robustness with\ndifferent degrees of visual corruption.\nSummary:\nWhen  considering  the  full  range  of  visually-\ncontinuous corruption,no NNs can match human accuracy,\nespecially for blur corruptions, though some can match\nhuman prediction consistency. Few NNs can outperform\nhumans in specific degrees of corruption.This highlights\na more substantial gap between human and NN robustness\nthan previously identified by [?]. By evaluating VCR using\nour human-centric metrics,  we can better understand the\nrobustness gap and work towards models closer to human\nperformance.\n4.3. Training with Data Augmentation\nVCR considers a different distribution of corruptions in\nthe images (i.e., continuous) than existing benchmarks (i.e.,\nselected  parameter  values),  so  model  performance  is  ex-\npected to improve once the model is fine-tuned on the new\ndistribution. We show a small retraining example to demon-\nstrate the usefulness of our benchmark in improving VCR.\nThe image classification model was fine-tuned with pa-\nrameters optimized using stochastic gradient descent (learn-\ning rate=0.001, momentum=0.9) and Cross-Entropy Loss.\nThe training set was generated from a subset sampled from\ntheIMAGENET[?] training set with a size of around 12,000,\nand typically five epochs were sufficient to see progress.\nWhile state-of-the-art NNs are optimized for the corruption\nfunctions inIMAGENET-C, for certain corruption functions,\nsuch as Motion Blur, Frost and Glass Blur,IMAGENET-C\nimages do not cover a wide range of visual changes, leaving\nroom for robustness improvement, as detailed in Tab. 2. Re-\nsults for SIN [?] and StandardR50 [?] are shown in Tab. 3;\naddittional details can be found in the codebase\n2\n.\nSummary:\nOur results indicate that retraining with VCR-\ngenerated images improves all metrics of NN model perfor-\nmance compared to human performance, even for models\noptimized forIMAGENET-C’s corruption functions. VCR\nintroduces  a  new  distribution  of  corruptions  that  models\nweren’t previously exposed to,  highlighting thatthe gap\nbetween human and NN robustness is larger than bench-\nmarks likeIMAGENET-Cdetect. VCR not only identifies\nthis gap but also helps to bridge it.",
        "metadata": {
          "source": "/Users/hk/Downloads/WACV_2025_Caroline_Huakun__Copy_.pdf",
          "pdf": {
            "version": "1.10.100",
            "info": {
              "PDFFormatVersion": "1.6",
              "IsAcroFormPresent": false,
              "IsXFAPresent": false,
              "Title": "",
              "Author": "",
              "Subject": "",
              "Keywords": "",
              "Creator": "LaTeX with hyperref",
              "Producer": "pdfTeX-1.40.26",
              "CreationDate": "D:20250127045718Z",
              "ModDate": "D:20250127045718Z",
              "Trapped": { "name": "False" }
            },
            "metadata": null,
            "totalPages": 8
          },
          "loc": { "pageNumber": 6 },
          "sha256": "ffbcb29742daf9184167d0e12e191e672b2d0662af402e8dfdea3f33117eb388"
        }
      }
    ],
    [
      "409af583-4e41-44eb-a5bb-be51e29b333c",
      {
        "pageContent": "(a)IMAGENET-C Gaussian Noise Accuracy.(b)Gaussian Noise\nˆ\nR\na\n.(c)Gaussian Noise\nˆ\nR\np\n.\nFigure 5. Comparison between IMAGENET-C and VCR with Gaussian Noise. Models discussed in the text are marked by a red triangle.\n(a)HMRIforR\na\n(b)MRSIforR\na\n(c) Estimated curvess\na\n(d)HMRIforR\np\n(e)MRSIforR\np\n(f) Estimated curvess\np\nFigure 6. VCR evaluation results for Gaussian Noise. Results include, for each NN, the estimated curvess\na\nands\np\n(representing how the\nperformance w.r.t. the robustness propertiesaandpdecreases as∆\nv\nincreases); and the correspondingHMRIandMRSIvalues. Results are\ncolored based on their category: Human, Vision Transformer, Supervised Learning, SWSL, Adversarial Training, CLIP.\n(a)HMRIforR\na\n(b)MRSIforR\na\n(c) Estimated curvess\na\n(d)HMRIforR\np\n(e)MRSIforR\np\n(f) Estimated curvess\np\nFigure 7. VCR evaluation results for Uniform Noise.\nResults for StandardR50 [?]Results for SIN [?]\nBefore RetrainingAfter RetrainingBefore RetrainingAfter Retraining\nAccuracyPrediction similarityAccuracyPrediction similarityAccuracyPrediction similarityAccuracyPrediction similarity\ncorruption function\nˆ\nR\na\nHMRIMRSI\nˆ\nR\np\nHMRIMRSI\nˆ\nR\na\nHMRIMRSI\nˆ\nR\np\nHMRIMRSI\nˆ\nR\na\nHMRIMRSI\nˆ\nR\np\nHMRIMRSI\nˆ\nR\na\nHMRIMRSI\nˆ\nR\np\nHMRIMRSI\nMedian Blur0.5320.6350.0000.5730.6730.0000.6940.8280.0030.7280.8540.0010.5220.6240.000.6050.7100.000.6500.7740.0040.7290.8520.004\nFrost0.4290.5210.0110.4730.5720.0120.5750.6900.0250.6780.8040.0310.4230.5120.0150.5130.6180.0160.5170.6250.0160.6470.7680.031\nGlass Blur0.4680.5690.0030.5020.6030.0030.6470.7700.0240.7440.8660.0340.3340.4070.0000.3970.4780.0000.5720.6870.0160.6840.8090.018\nNote: all numbers are rounded.\nTable 3. VCR comparison before and after retraining. Red indicates improvement.\n4.4. Visually Similar Corruption Functions\nOur experiments revealed the existence ofvisually similar\ncorruption functions, which can potentially reduce experi-\nment costs and enhance our understanding of differences be-\ntween humans and NNs. Different corruptions affect aspects\nlike color, contrast, and noise, influencing human perception\nin varied ways [?]. For example, Gaussian and Impulse noise\nmay have similar visual effects, making them hard for an av-\nerage human to distinguish. We call such functionsvisually\nsimilar. We postulate that since visually similar functions,",
        "metadata": {
          "source": "/Users/hk/Downloads/WACV_2025_Caroline_Huakun__Copy_.pdf",
          "pdf": {
            "version": "1.10.100",
            "info": {
              "PDFFormatVersion": "1.6",
              "IsAcroFormPresent": false,
              "IsXFAPresent": false,
              "Title": "",
              "Author": "",
              "Subject": "",
              "Keywords": "",
              "Creator": "LaTeX with hyperref",
              "Producer": "pdfTeX-1.40.26",
              "CreationDate": "D:20250127045718Z",
              "ModDate": "D:20250127045718Z",
              "Trapped": { "name": "False" }
            },
            "metadata": null,
            "totalPages": 8
          },
          "loc": { "pageNumber": 7 },
          "sha256": "c19ae6410ab85759fbba1eb7c0ed49cb4f7a62759288ade5a287714e57f17823"
        }
      }
    ],
    [
      "2f47ea98-2701-4f84-a139-c76a2a261e84",
      {
        "pageContent": "(a)Blur classs\nh\na\n(b)Noise classs\nh\na\n(c)Dissimilars\nh\na\nFigure 8. Comparing human performance spline curvess\nh\na\nfor similar and dissimilar corruption functions. For each curve, the coloured\nregion around the curve is the83%confidence interval used for comparison of similarity. Sees\nh\np\nin App.??.\nby definition, affect human perception similarly, they would\naffect human robustness similarly as well. This suggests that\nhuman data for one function can be reused for other similar\nfunctions, potentially lowering experiment costs.\nSince VCR is estimated with the spline curvess\nh\na\nands\nh\np\n,\nif the difference among the curves of a set of functions is\nstatistically insignificant, human data (i.e., the spline curves)\ncan be reused among the functions in this set. In Fig. 8, we\nplot the smoothed spline curvess\nh\na\nands\nh\np\nobtained for all\n14 corruption functions included in our experiments. We can\nobserve that, for all corruption functions shown, human per-\nformance decreases slowly for small values of visual degrade\n(∆\nv\n), but once∆\nv\nreaches a turning point, human perfor-\nmance starts decreasing more rapidly.  Then, we observe\nthat spline curves obtained for certain blur and noise trans-\nformations have similar shapes, while those for dissimilar\ntransformations start decreasing at different turning points\nwith different slopes. More specifically, the differences be-\ntween two spline curves are statistically insignificant if their\n83%confidence intervals overlap [?].\nSummary:By  checking  statistical  significance  with83%\nconfidence interval for each corruption function, we empir-\nically observed two classes of visually similar corruptions\nin our experiments with humans: (1) the noise class: Shot\nNoise, Impulse Noise, Gaussian Noise, and Uniform Noise;\nand (2) the blur class:  Blur, Median Blur, Gaussian Blur,\nGlass blur, Defocus Blur.  The remaining corruptions are\ndissimilar (see Fig. 8).\nNN Robustness for Visually Similar Corruption Func-\ntions.Due to fundamental differences between humans and\nNNs, such as computational power, NNs may respond dif-\nferently to visually similar corruptions.  VCR allows us to\nempirically analyze these differences. For instance, during\ndeployment, NNs may encounter noise with unknown distri-\nbutions (e.g., Uniform, Gaussian, Poisson) that do not affect\nhumans as shown in Fig. 8, but could pose safety concerns\nif NNs are particularly sensitive to specific distributions.\nFor example, Gaussian Noise and Uniform Noise (visu-\nally similar) both add noise to images but from different\ndistributions.  Our results in Fig. 6 and Fig. 7 suggest that\nthe NNs detect the distribution difference. Models generally\nexhibit higherHMRIandMRSIvalues for Uniform Noise\ncompared to Gaussian Noise.   While performance differ-\nences are not statistically significant with low corruption\n(∆\nv\n<0.8), models perform better with Uniform Noise than\nGaussian Noise at higher corruption levels (∆\nv\nbetween\n[0.8..1.0]). Studying VCR helps analyze how different noise\ndistributions impact NN performance, an impractical task\nwith human data for all possible distributions.  Identifying\nvisually similar corruption functions and reusing human data\ncan significantly reduce experimental costs.\nIdentifying Visually Similar Transformations.We pro-\npose  a  simple  method  for  identifying  classes  of  visually\nsimilar corruptions by determining if humans can distin-\nguish between them.  Participants view corrupted images\nand indicate if they believe the corruptions are the same or\ndifferent. By measuring accuracy across multiple trials, we\nuse a binomial test to assess statistical significance.  Our\nmethod can detect visually similar transformations quickly,\nreducing experiment time from about 5.55 hours with 2,000\nimages and five participants to just 5 minutes.\nLimitation:Our method’s results depend on participants hav-\ning normal eyesight and basic knowledge of image corrup-\ntions, and may not always accurately identify visually similar\ntransformations. For instance, transformations with differ-\nent visual effects but similar impacts on human robustness\nmight not be detected.  Despite these limitations, we hope\nour approach encourages further research into how NNs and\nhumans respond differently to corruptions.\n5. Conclusion\nIn this paper, we introducevisually-continuous corruption\nrobustness(VCR) and two novel human-aware metrics for\nevaluating NNs.  Our findings reveala larger robustness\ngap between humans and NNs than previously detected,\nparticularly for blur corruptions. We emphasize that using a\ncomprehensive range of visual changes is crucial for accurate\nrobustness estimation, asinsufficient coverage can lead to\nbiased results. We also identify classes of image corruptions\nthat similarly affect human perception, which can reduce the\ncost of measuring human robustness and assessing gaps with\ncomputational models. While our study focused on object\nrecognition, human and machine vision comparisons could\nextend to other aspects like neural data [?,?], contrasting\nGestalt effects [?], object similarity judgments [?], or mid-\nlevel properties [?].   We hope our results inspire further\nrobustness research and offer our benchmark datasets and\ncode as open source.",
        "metadata": {
          "source": "/Users/hk/Downloads/WACV_2025_Caroline_Huakun__Copy_.pdf",
          "pdf": {
            "version": "1.10.100",
            "info": {
              "PDFFormatVersion": "1.6",
              "IsAcroFormPresent": false,
              "IsXFAPresent": false,
              "Title": "",
              "Author": "",
              "Subject": "",
              "Keywords": "",
              "Creator": "LaTeX with hyperref",
              "Producer": "pdfTeX-1.40.26",
              "CreationDate": "D:20250127045718Z",
              "ModDate": "D:20250127045718Z",
              "Trapped": { "name": "False" }
            },
            "metadata": null,
            "totalPages": 8
          },
          "loc": { "pageNumber": 8 },
          "sha256": "a508c2976b25ed28733d26d2cd7601ee67aff3ad8cc0f1faf221e2ba23f325d4"
        }
      }
    ]
  ],
  {
    "0": "98414fd1-37b6-4cf7-8110-0308f567ae64",
    "1": "70e44c06-e96d-45cc-99e8-808f940f09eb",
    "2": "009803b4-aaa1-4c00-8420-58e03d6591ec",
    "3": "ed0d864f-659e-4c39-9a43-d2ed536a9aa3",
    "4": "d1252769-b655-4dda-92ee-c65b4c81adc6",
    "5": "8f5d9f88-4201-4870-867c-8b7cd086ae84",
    "6": "e0c5e43f-5d53-4fcd-ae6e-187eaa6067f5",
    "7": "e47c0840-6b15-4c50-bdbc-f07bad9a9777",
    "8": "94934e87-393a-4f74-91ed-cd7b1490d89f",
    "9": "4443b394-cd6c-4642-8c18-4e02acc3f29e",
    "10": "71264655-6296-47c6-ba77-0b41039dd320",
    "11": "8eb2aac7-65de-40bc-a2bf-35971f5a3c15",
    "12": "53683cba-b791-4174-8b7a-fb83b3b0e619",
    "13": "a57847a1-d9d3-49ca-b4ad-ec699d19733f",
    "14": "bbe73410-cfaa-4098-ae7f-31cc1167980a",
    "15": "fb2008e6-b807-4f0c-87af-ee16dd045212",
    "16": "c01af585-d8b9-4819-9a7a-ed21c9890e0a",
    "17": "8a4bdf4b-8688-4186-9675-5ded43ae1947",
    "18": "d2c78f03-36a2-4906-8fa6-3a2b4544b611",
    "19": "bcb1ce57-254c-4e78-a735-be0cc4fd988e",
    "20": "409af583-4e41-44eb-a5bb-be51e29b333c",
    "21": "2f47ea98-2701-4f84-a139-c76a2a261e84"
  }
]
